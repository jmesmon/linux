
#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

#include <linux/debugfs.h>
#include <linux/printk.h>
#include <linux/module.h> /* MODULE_* */
#include <linux/init.h> /* module_{init,exit} */
#include <linux/timer.h> /* DEFINE_TIMER, mod_timer */
#include <linux/fsnotify.h> /* fsnotify, FS_MODIFY, fsnotify_parent, FSNOTIFY_EVENT_INODE */

#include <linux/mmzone.h> /* __nr_to_section(), pfn_to_nid(),  */


/* pfn_to_nid_lookup - searches through the dynamic topology manager for a
 *                     pfn's nid.
 *
 * Returns the (positive) nid a pfn belongs to, or -1 when a node does not
 * exsist.
 *
 * Used to determine migration need for DNUMA
 */
int pfn_to_node_lookup(unsigned long pfn)
{
	/* XXX: impliment */
	return pfn_to_nid(pfn);
}


static struct dentry *root_dentry, *do_it_dentry, *delay_dentry;
static u8 do_it;
static u32 delay = 1000;

/* Potential sources of inspiration:
 * - mm/page_cgroup.c - lookup_page_cgroup()
 * - mm/memory_hotplug.c - register_memory_section()
 */

/* Potential issues:
 * - all of the potential sources of inspiration.
 * - per-node & per-zone statistic tracking.
 *   - Where? What? (I haven't been able to find any yet).
 * - memory cgroups
 * - transparent huge page: multiple pages tracked as a single page for lru
 *   purposes. lru is per zone. [half the page in one lru half in another, et
 *   c.]
 */

/* == Not Planned ==
 * Strategy for mem_sections split across nodes:
 * - use an embedded section number (which is included in all cases but SPARSEMEM_VMEMMAP)
 * - change {pfn,page}_to_{nid,node} to use the embedded number [when a particular set of flags are set? always?].
 * - audit users of page_to_section & __page_to_section.
 *   - __page_to_section() - uses a shift to calculate section_nr via __page_to_section_nr()
 *   - page_to_section() - uses the section in page flags. does not appear to be defined when the section is not in page flags.
 * - potentially make sure page_to_section() is defined in all cases, and use that as the proper api to access the section number
 *   - do we need to include a method which _always_ uses the section_nr from page flags?
 * - Q: where are we going to keep track of the 'backup' mem_sections?
 * - Worst case is an extra struct mem_section per node.
 *   - How costly is this?
 */

/* We need to split sections when defined(NODE_NOT_IN_PAGE_FLAGS).
 * In that case, sometimes we have a section number in the page flags, so we can change the section number there.
 *	- code presently does not appear to use the section number stored in page flags, instead a shift is used to calculate the section in page_to_section_nr().
 */

/* Do the page flags ever look like:  | ZONE                  | Flags |
 *
 * The following are the 3 mentioned layouts for the page flags:
 * No sparsemem or sparsemem vmemmap: |       NODE     | ZONE | ... | FLAGS |
 * classic sparse with space for node:| SECTION | NODE | ZONE | ... | FLAGS |
 * classic sparse no space for node:  | SECTION |     ZONE    | ... | FLAGS |
 */

/* register_memory_notifier() [hotplug_memory_notifier()] */

/* TODO: find where all pages in a section are assumed to have the same node */

static inline struct zone *get_zone(unsigned long nid, enum zone_type zonenum)
{
	return &NODE_DATA(nid)->node_zones[zonenum];
}
/*
 * MAX_NR_ZONES generated by kbuild. (bounds.h)
 */

/* Assumptions:
 * - zone number is unchanged.
 */

/* removes pages from a zone's LRU list and attaches them to @list
 * - all pages belong to the same 'zone'.
 * - returns the number of pages which were removed from the lru lists.
 *
 *  mm/swap.c:static void __page_cache_release(struct page *page)::
 *  - it checks PageLRU prior to locking the zone.
 *  - a change in PageLRU between the check & locking is considered a VM_BUG.
 *
 * XXX: how do we re-add pages to the new lruvec?
 * XXX: pagevec??
 * How does this differ from using 'isolate_page()'
 * XXX: it looks almost identical.
 */
static int remove_pfn_range_from_lru(struct zone *zone, unsigned long start_pfn, size_t page_ct, struct list_head *list)
{
	unsigned long flags, pfn, ct = 0;
	struct page *page;
	for (pfn = start_pfn; pfn < start_pfn + page_ct; pfn += hpage_size(page)) {
		page = pfn_to_page(pfn);
		if (PageLRU(page)) {
			/* The double check of PageLRU() is an attempt to decrease contention on zone->lru_lock */
			struct lruvec *lruvec = mem_cgroup_page_lruvec(page, zone);
			spin_lock_irqsave(&zone->lru_lock, flags);
			if (PageLRU(page)) {
				int lru = page_lru();
				ClearLRU(page);
				del_page_from_lru_list(page, lruvec, lru);
				list_add(&page->list, list);
				ct++;
			}
			spin_lock_irqrestore(&zone->lru_lock, flags);
		}
	}

	return ct;
}

static void add_pages_to_lru(new_zone, lru_list) {}

static int move_pfn_range_to_nid(unsigned long start_pfn, size_t page_ct, int node_id)
{
	unsigned long end_pfn = start_pfn + page_ct - 1; /* inclusive range */
	unsigned long zone_start_pfn = start_pfn; /* the pfn that begins the
						     currently processed
						     'struct zone' */
	unsigned long pfn;
	size_t rem_pages = page_ct;
	enum zone_type prev_zone = MAX_NR_ZONES; /* tracks the zone we are
						    currently accumulating
						    pages from. Inside the pfn
						    loop, it is the zone the
						    previous pfn belonged to.
						    */

	if (!node_possible(node_id))
		return -EINVAL;

	/* TODO: if node_id is possible but not 'created', create it. */
	/* XXX: lock nodes so the one we want doesn't disappear */
	/*      What can remove/offline nodes? */

	for(pfn = start_pfn; pfn < start_pfn + page_ct;) {
		if (!pfn_valid(pfn) || !pfn_present(pfn)) {
			/* XXX: what is the right way to handle this? */
			/* - do we need to skip pfn's that are not 'valid' */
			/* - what does it mean for a pfn to be 'present'? */
			pr_devel("pfn %lu not valid or not present", pfn);
			continue;
		}

#if defined(NODE_NOT_IN_PAGE_FLAGS) && defined(SECTION_IN_PAGE_FLAGS)
		/* NODE = no, SECTION = yes*/

		/* does the current range cover all pfns in the section? */
		/* XXX: what should happen if we are migrating a second time? */
		/* is the pfn at the start of a section? */
		/* we use the calculated section number here to see if */
		{
			bool have_partial_section, short_start, short_end;
			unsigned long sec_nr = pfn_to_section_nr(pfn);
			if (sec_nr != pfn_to_section_nr(pfn + PAGES_PER_SECTION - 1)) {
				/* pfn does not start at the start of a section, perform a partial section clone. */
				have_partial_section = true;
				short_start = true;
			}

			/* does pfn + remaining ct not reach the end of a section? */
			if (PAGES_PER_SECTION > (start_pfn + page_ct) - pfn) {
				have_partial_section = true;
				short_end = true;
			}

			if (!have_partial_section) {
				/* YAY! */
				set_section_nid(sec_nr, nid);
			} else {
				/* TODO: 'allocate' an unused spot in a section */
				/*  - Are other things tracked by section
				 *    number (in the way nid is: ie, not in the
				 *    mem_section struct)?
				 */
# error "Dynamic Numa requires that the NODE is in page flags. Try a 64bit platform."
			}
		}
#elif !defined(NODE_NOT_IN_PAGE_FLAGS)
		/* NODE = yes, SECTION = maybe */
		/* Consider mm/page_cgroup.c */
		{
			/* XXX: when do pfn_to_page() mappings change? */
			struct page *page = pfn_to_page(pfn);
			struct zone *zone = page_zone(page);
			/* Nodes are currently only set by memmap_init_zone().
			 * Called by:
			 *  - memmap_init()
			 *  - (on ia64) virtual_memmap_init()
			 *  - memory_hotplug.c::__add_node()
			 *    - __add_section()
			 *      - __add_pages()
			 *
			 */
			unsigned long old_nid = page_to_nid(p);
			rem_pages--;
			set_page_node(page, nid);
			if (prev_zone == NULL)
				prev_zone = zone;
			else if (prev_zone != zone) {
				unsigned long flags;
				unsigned long lru_pfn;
				size_t pages_removed = pfn - zone_start_pfn;
				LIST_HEAD(lru_list);
				/* XXX: should be done when the pages are removed from the free_list. */
				/* XXX: active_list and inactive_list - should
				 *      we bother moving these pages now or wait for
				 *      the deallocation to handle it?
				 * - waiting could mean that zone statistics
				 *   are incorrect, __free_pages() will need to
				 *   correct them, which means __free_pages()
				 *   needs to know the original node, which it
				 *   doesn't.
				 * - If we correct the statistics but wait for
				 *   __free_pages() to move them to the right
				 *   node->zone, things that act on the
				 *   statistics (kswapd) might not perform
				 *   optimally.
				 * - If we try to move the pages now, locking
				 *   is a pain. Oh well. */
				/* TODO: move pages between free lists here
				 * OR change __alloc_pages to migrate them between free lists  */
				/* XXX: locking?? */
				/* some form of {__,}isolate_page{s,}() */
				remove_pfn_range_from_lru(prev_zone, zone_start_pfn, pages_removed, lru_list);

				/* XXX: Is this always what we want? What if one node ends being deficient in one of the zones?
				 *      Is that fixable by choosing a different 'new_zone'? */
				struct zone *new_zone = &new_node->zones[prev_zoneid];
				add_pages_to_lru(new_zone, lru_list);

				/* XXX: what other statistics need updating? */
				/* FIXME: this is wrong: not all pages are free. */
				__mod_zone_page_state(prev_zone, NR_FREE_PAGES, -pages_removed);
				__mod_zone_page_state(new_zone,  NR_FREE_PAGES,  pages_removed);

				zone_start_pfn = pfn;
				prev_zone = zone;
			}
			/* Inc. here to avoid counting the just processed page in the stats for the previous zone. */
			pfn += hpage_nr_pages(page);
		}
#else
# error "Dynamic Numa requires that the NODE is in page flags. Try a 64bit platform."
#endif
		/* if yes, move_mem_section_to_nid() */
		/* if no,  move_partial_mem_section_to_nid() */
		/* pfn += SECTION_SIZE // is SECTION_SIZE defined/correct when nid is not stored in sections? */
	}

	return 0;
}

static int debugfs_u8_get(void *data, u64 *val)
{
	*val = *(u8 *)data;
	return 0;
}

#define DEFINE_WATCHED_U8(___var)				\
	static int ___var ## _watch_set(void *data, u64 val)	\
	{							\
		u8 old_val = *(u8 *)data;			\
		int ret = ___var ## _watch(old_val, val);	\
		if (!ret)					\
			*(u8 *)data = val;			\
		return ret;					\
	}							\
	DEFINE_SIMPLE_ATTRIBUTE(___var ## _fops, debugfs_u8_get, ___var ## _watch_set, "%llu\n");

static void change_back(unsigned long arg)
{
	pr_devel("resetting 'do-it'");
	do_it = 0;
	fsnotify_modify_dentry(do_it_dentry);
}

/* Try using hrtimer_* ? */
static struct timer_list change_back_tl = TIMER_DEFERRED_INITIALIZER(change_back, 0, 0);

static int do_it_watch(u8 old_val, u8 new_val)
{
	pr_devel("do-it: %d => %d.", old_val, new_val);
	if (old_val == 0 && new_val == 1) {
		pr_devel("setting timer for %d ms.", delay);
		if (!timer_pending(&change_back_tl)) {
			mod_timer(&change_back_tl,  jiffies + msecs_to_jiffies(delay));
		} else {
			pr_devel("timer pending, should never happen.");
			return -EIO;
		}
		return 0;
	}
	return -EACCES;
}
DEFINE_WATCHED_U8(do_it);

static int __init mod_init(void)
{
	if (!debugfs_initialized()) {
		pr_devel("debugfs not registered or disabled.");
		return 0;
	}

	root_dentry = debugfs_create_dir("debugfs-test", NULL);
	if (!root_dentry) {
		pr_devel("failed to create dir");
		return 0;
	}

	do_it_dentry = debugfs_create_file("do-it", 0700, root_dentry, &do_it, &do_it_fops);
	if (!do_it_dentry) {
		pr_devel("failed to create do-it");
		return 0;
	}

	delay_dentry = debugfs_create_u32("delay", 0700, root_dentry, &delay);
	if (!delay_dentry) {
		pr_devel("failed to create delay");
		return 0;
	}

	pr_devel("initialized: do-it: %d, delay: %d", do_it, delay);

	return 0;
}

static void __exit mod_exit(void)
{
	debugfs_remove_recursive(root_dentry);
}

module_init(mod_init);
module_exit(mod_exit);

MODULE_AUTHOR("Cody P Schafer <cody@linux.vnet.ibm.com>");
MODULE_DESCRIPTION("Dynamic Numa Support.");
MODULE_LICENSE("GPL");
